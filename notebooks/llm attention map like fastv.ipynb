{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as Colormap\n",
    "from matplotlib.colors import LogNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(multihead_attention,output_path=\"atten_map_1.png\",title=\"\"):\n",
    "    # Assuming the input is a numpy array of shape (1, num_heads, n_tokens, n_tokens)\n",
    "    # First, we average the attention scores over the multiple heads\n",
    "    averaged_attention = torch.mean(multihead_attention, axis=1)[0].float()# Shape: (n_tokens, n_tokens)\n",
    "    \n",
    "    # pooling the attention scores  with stride 20\n",
    "    averaged_attention = torch.nn.functional.avg_pool2d(averaged_attention.unsqueeze(0).unsqueeze(0), 20, stride=20).squeeze(0).squeeze(0)\n",
    "    \n",
    "    cmap = plt.cm.get_cmap(\"viridis\")\n",
    "    plt.figure(figsize=(5, 5),dpi=400)\n",
    "\n",
    "    # Log normalization\n",
    "    log_norm = LogNorm(vmin=0.0007, vmax=averaged_attention.max())\n",
    "\n",
    "    averaged_attention = averaged_attention.cpu().numpy()\n",
    "\n",
    "\n",
    "    ax = sns.heatmap(averaged_attention,\n",
    "                cmap=cmap,  # custom color map\n",
    "                norm=log_norm,  # \n",
    "                # cbar_kws={'label': 'Attention score'},\n",
    "                )\n",
    "    \n",
    "    # remove the x and y ticks\n",
    "    \n",
    "    # replace the x and y ticks with string\n",
    "\n",
    "    x_ticks = [str(i*20) for i in range(0,averaged_attention.shape[0])]\n",
    "    y_ticks = [str(i*20) for i in range(0,averaged_attention.shape[0])]\n",
    "    ax.set_xticks([i for i in range(0,averaged_attention.shape[0])])\n",
    "    ax.set_yticks([i for i in range(0,averaged_attention.shape[0])])\n",
    "    ax.set_xticklabels(x_ticks)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    # change the x tinks font size\n",
    "    plt.xticks(fontsize=3)\n",
    "    plt.yticks(fontsize=3)\n",
    "    \n",
    "    # make y label vertical\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=90)     \n",
    "    \n",
    "    plt.title(title)\n",
    "    # # tight layout\n",
    "    # plt.savefig(output_path, bbox_inches='tight')\n",
    "    # # plt.show()\n",
    "\n",
    "    # top_five_attentions = []\n",
    "    # for row in averaged_attention:\n",
    "    #     # Use torch.topk to get the top 5 values and their indices\n",
    "    #     top_values, top_indices = torch.topk(row, 10)\n",
    "    #     # Convert to lists and append to the overall list\n",
    "    #     top_five_line = list(zip(top_indices.tolist(), top_values.tolist()))\n",
    "    #     top_five_attentions.append(top_five_line)\n",
    "        \n",
    "    # return top_five_attentions,averaged_attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def analyze_vit_and_llm_attentions(batch_id, vit_attention, llm_attention, vit_to_llm_mapping):\n",
    "    \n",
    "    vit_to_llm_mapping = vit_to_llm_mapping[batch_id]\n",
    "    llm_attention = torch.stack([i[batch_id] for i in llm_attention])  # [num_layers, num_heads, num_context, num_context]\n",
    "    vit_attention = torch.stack([i[batch_id] for i in vit_attention])  # [num_layers, num_heads, num_patches, num_patches]\n",
    "\n",
    "    num_tokens = vit_to_llm_mapping.shape[-1]\n",
    "\n",
    "    llm_attention = llm_attention.sum(dim=1)  # [num_layers, num_context, num_context]\n",
    "    vit_attention = vit_attention.sum(dim=1)  # [num_layers, num_patches, num_patches]\n",
    "\n",
    "    llm_attention = llm_attention.sum(dim=1)  # [num_layers, num_context]\n",
    "    vit_attention = vit_attention[:,0,-num_tokens:]  # [num_layers, num_tokens]\n",
    "\n",
    "    llm_attention = llm_attention[:, vit_to_llm_mapping]\n",
    "\n",
    "\n",
    "    vit_num_layers = vit_attention.shape[0]\n",
    "    llm_num_layers = llm_attention.shape[0]\n",
    "\n",
    "    # to cpu and numpy\n",
    "    vit_attention = vit_attention.cpu().numpy()\n",
    "    llm_attention = llm_attention.cpu().numpy()\n",
    "\n",
    "    kendall_matrix = np.zeros((vit_num_layers, llm_num_layers))\n",
    "\n",
    "    for vit_layer in range(vit_num_layers):\n",
    "        for llm_layer in range(llm_num_layers):\n",
    "            if vit_layer == 12 and llm_layer == 0:\n",
    "                # reshape vit_layer to 24x24 and show the matrix\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                sns.heatmap(vit_attention[vit_layer].reshape(24, 24), annot=False, cmap='coolwarm', cbar=False)\n",
    "            vit_sorted_index = np.argsort(vit_attention[vit_layer])\n",
    "            llm_sorted_index = np.argsort(llm_attention[llm_layer])\n",
    "            kendall, _ = kendalltau(vit_sorted_index, llm_sorted_index)\n",
    "            kendall_matrix[vit_layer, llm_layer] = kendall\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(kendall_matrix, annot=False, cmap='coolwarm', cbar=True)\n",
    "    plt.title('Kendall Tau')\n",
    "    plt.xlabel('LLM Layers')\n",
    "    plt.ylabel('ViT Layers')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def print_vit_vs_llm_attention_similarity(batch_id, vit_attention, llm_attention, vit_to_llm_mapping):\n",
    "    \n",
    "    vit_to_llm_mapping = vit_to_llm_mapping[batch_id]\n",
    "    llm_attention = torch.stack([i[batch_id] for i in llm_attention])  # [num_layers, num_heads, num_context, num_context]\n",
    "    vit_attention = torch.stack([i[batch_id] for i in vit_attention])  # [num_layers, num_heads, num_patches, num_patches]\n",
    "\n",
    "    num_tokens = vit_to_llm_mapping.shape[-1]\n",
    "\n",
    "    llm_attention = llm_attention.sum(dim=1)  # [num_layers, num_context, num_context]\n",
    "    vit_attention = vit_attention.sum(dim=1)  # [num_layers, num_patches, num_patches]\n",
    "\n",
    "    llm_attention = llm_attention.sum(dim=1)  # [num_layers, num_context]\n",
    "    vit_attention = vit_attention[:,0,-num_tokens:]  # [num_layers, num_tokens]\n",
    "\n",
    "    llm_attention = llm_attention[:, vit_to_llm_mapping]\n",
    "\n",
    "\n",
    "    vit_num_layers = vit_attention.shape[0]\n",
    "    llm_num_layers = llm_attention.shape[0]\n",
    "\n",
    "    # to cpu and numpy\n",
    "    vit_attention = vit_attention.cpu().numpy()\n",
    "    llm_attention = llm_attention.cpu().numpy()\n",
    "\n",
    "    for vit_layer in range(vit_num_layers):\n",
    "        if vit_layer == -2:\n",
    "            # reshape vit_layer to 24x24 and show the matrix\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            sns.heatmap(vit_attention[vit_layer].reshape(24, 24), annot=False, cmap='coolwarm', cbar=False)\n",
    "        vit_sorted_index = np.argsort(vit_attention[vit_layer])\n",
    "        kendall_sum = 0\n",
    "        for llm_layer in range(llm_num_layers):\n",
    "            llm_sorted_index = np.argsort(llm_attention[llm_layer])\n",
    "            kendall, _ = kendalltau(vit_sorted_index, llm_sorted_index)\n",
    "            kendall_sum += kendall\n",
    "        \n",
    "        print(f'ViT Layer {vit_layer} Kendall Tau: {kendall_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def print_vit_vs_llm_attention_similarity(batch_id, vit_attention, llm_attention, vit_to_llm_mapping):\n",
    "    \n",
    "    vit_to_llm_mapping = vit_to_llm_mapping[batch_id]\n",
    "    llm_attention = torch.stack([i[batch_id] for i in llm_attention])  # [num_layers, num_heads, num_context, num_context]\n",
    "    vit_attention = torch.stack([i[batch_id] for i in vit_attention])  # [num_layers, num_heads, num_patches, num_patches]\n",
    "\n",
    "    num_tokens = vit_to_llm_mapping.shape[-1]\n",
    "\n",
    "    llm_attention = llm_attention.sum(dim=1)  # [num_layers, num_context, num_context]\n",
    "    vit_attention = vit_attention.sum(dim=1)  # [num_layers, num_patches, num_patches]\n",
    "\n",
    "    llm_attention = llm_attention.sum(dim=1)  # [num_layers, num_context]\n",
    "    vit_attention = vit_attention[:,0,-num_tokens:]  # [num_layers, num_tokens]\n",
    "\n",
    "    llm_attention = llm_attention[:, vit_to_llm_mapping]\n",
    "\n",
    "\n",
    "    vit_num_layers = vit_attention.shape[0]\n",
    "    llm_num_layers = llm_attention.shape[0]\n",
    "\n",
    "    # to cpu and numpy\n",
    "    vit_attention = vit_attention.cpu().numpy()\n",
    "    llm_attention = llm_attention.cpu().numpy()\n",
    "\n",
    "    for vit_layer in range(vit_num_layers):\n",
    "        if vit_layer == -2:\n",
    "            # reshape vit_layer to 24x24 and show the matrix\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            sns.heatmap(vit_attention[vit_layer].reshape(24, 24), annot=False, cmap='coolwarm', cbar=False)\n",
    "        vit_sorted_index = np.argsort(vit_attention[vit_layer])\n",
    "        kendall_sum = 0\n",
    "        for llm_layer in range(llm_num_layers):\n",
    "            llm_sorted_index = np.argsort(llm_attention[llm_layer])\n",
    "            kendall, _ = kendalltau(vit_sorted_index, llm_sorted_index)\n",
    "            kendall_sum += kendall\n",
    "        \n",
    "        print(f'ViT Layer {vit_layer} Kendall Tau: {kendall_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llava-hf/llava-1.5-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    attn_implementation=\"eager\",\n",
    ").to(0)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://picsum.photos/400\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open('image.jpg', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Image successfully retrieved and saved.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve image. HTTP Status code: {response.status_code}\")\n",
    "raw_image = Image.open('image.jpg')\n",
    "raw_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"USER: <image>\\nDescribe the image in details\\nASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=256,\n",
    "        use_cache=True,\n",
    "        output_attentions=True,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "output_text = processor.decode(output_ids['sequences'][0], skip_special_tokens=False)\n",
    "print(output_text)\n",
    "inputs = processor(output_text, raw_image, return_tensors='pt').to(0, torch.float16)\n",
    "with torch.inference_mode():\n",
    "    output = model(**inputs, output_attentions=True, return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_attention = output.attentions\n",
    "vit_attention = output.vit_attentions\n",
    "vit_to_llm_mapping = output.vit_to_llm_mapping\n",
    "\n",
    "# make each numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

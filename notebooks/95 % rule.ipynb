{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as Colormap\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def count_top_nprcnt_contribution_llm(llm_attention, vision_tokens_index, topk=100):\n",
    "    # attn: tuple of torch tensor [1, num_heads, n_all_tokens, n_all_tokens]\n",
    "    # vision_tokens_index: torch tensor [n_vision_tokens]\n",
    "    num_layers = len(llm_attention)\n",
    "    text_token_start = vision_tokens_index[-1]+1\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        attn = llm_attention[layer][0]  # [num_heads, n_all_tokens, n_all_tokens]\n",
    "        vision_attn = attn[:, text_token_start:, vision_tokens_index]  # [num_heads, n_text_tokens, n_vision_tokens]\n",
    "        vision_attn = vision_attn.sum(dim=0) # [n_text_tokens, n_vision_tokens]\n",
    "        vision_attn = vision_attn.sum(dim=0) # [n_vision_tokens]\n",
    "\n",
    "        vision_attn_sorted, _ = torch.sort(vision_attn, descending=True)\n",
    "        # sum of topk tokens\n",
    "        vision_attn_topk_sum = vision_attn_sorted[:topk].sum()\n",
    "        # sum of all tokens\n",
    "        vision_attn_sum = vision_attn.sum()\n",
    "\n",
    "        # attein by topk\n",
    "        print(f\"Layer {layer}: {vision_attn_topk_sum/vision_attn_sum*100:.2f}%\")\n",
    "\n",
    "\n",
    "def count_top_npercents_contribution(vit_attention, topk=100):\n",
    "    # vit_attention: tuple of torch tensor [1, num_heads, n_all_tokens, n_all_tokens]\n",
    "    num_layers = len(vit_attention)\n",
    "    n_all_tokens = vit_attention[0].shape[-1]\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        attn = vit_attention[layer][0] # [num_heads, n_all_tokens, n_all_tokens]\n",
    "        attn = attn.sum(dim=0) # [n_all_tokens, n_all_tokens]\n",
    "        cls_attn = attn[0, 1:] # [n_all_tokens]\n",
    "        \n",
    "        cls_attn_sorted, _ = torch.sort(cls_attn, descending=True)\n",
    "        # sum of topk tokens\n",
    "        cls_attn_topk_sum = cls_attn_sorted[:topk].sum()\n",
    "        # sum of all tokens\n",
    "        cls_attn_sum = cls_attn.sum()\n",
    "\n",
    "        # attein by topk\n",
    "        print(f\"Layer {layer}: {cls_attn_topk_sum/cls_attn_sum*100:.2f}%\")\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def visualize_attention(multihead_attention, layer=31, stride=1, vision_tokens_index=[]):\n",
    "    # Move the attention map to CPU and select the specified layer\n",
    "    multihead_attention = multihead_attention[layer].cpu()  # Shape: (1, num_heads, n_tokens, n_tokens)\n",
    "    \n",
    "    # Compute the average across the heads and reshape\n",
    "    averaged_attention = torch.mean(multihead_attention, dim=1)[0].float()  # Shape: (n_tokens, n_tokens)\n",
    "    \n",
    "    # Pooling to reduce size\n",
    "    averaged_attention = torch.nn.functional.avg_pool2d(averaged_attention.unsqueeze(0).unsqueeze(0), stride, stride).squeeze(0).squeeze(0)\n",
    "    \n",
    "    # Color mapping\n",
    "    cmap = plt.cm.get_cmap(\"viridis\")\n",
    "    \n",
    "    # Figure settings\n",
    "    plt.figure(figsize=(5, 5), dpi=100)\n",
    "    \n",
    "    # Normalization for color mapping\n",
    "    log_norm = LogNorm(vmin=0.0007, vmax=averaged_attention.max())\n",
    "    \n",
    "    # Heatmap plot\n",
    "    ax = sns.heatmap(averaged_attention, cmap=cmap, norm=log_norm)\n",
    "    \n",
    "    # Process vision tokens index tensor and add patches to mark them\n",
    "    vision_tokens_index = vision_tokens_index.cpu().numpy()  # Convert index tensor to CPU and numpy array\n",
    "\n",
    "    # apply stride to vision tokens index\n",
    "    vision_tokens_index = vision_tokens_index // stride\n",
    "    \n",
    "    # Adding red box to mark vision tokens\n",
    "    for idx in vision_tokens_index:\n",
    "        ax.add_patch(plt.Rectangle((idx, idx), 1, 1, fill=True, edgecolor='red', lw=1))\n",
    "\n",
    "\n",
    "    ax.set_xlabel('Token Index')\n",
    "    ax.set_ylabel('Token Index')\n",
    "\n",
    "    # do not show ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Title\n",
    "    plt.title(f'Attention Map Visualization for {layer+1}th layer')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    attn_implementation=\"eager\",\n",
    ").to(0)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run down there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://picsum.photos/400\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open('image.jpg', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Image successfully retrieved and saved.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve image. HTTP Status code: {response.status_code}\")\n",
    "# raw_image = Image.open('./license.png')\n",
    "raw_image = Image.open('image.jpg')\n",
    "raw_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"USER: <image>\\nTell me the story of two friends and the bear\\nASSISTANT:\"\n",
    "prompt = \"USER: <image>\\nDescribe the image in detail\\nASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.fast_vlm_config = {\n",
    "    \"spatial_budget\": 0,\n",
    "    \"alpha_vision_token_budget\": 0.2,\n",
    "    \"beta_sub_images_budget\": 0.2,\n",
    "    \"clip_attn_layer\": 22,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start time\n",
    "start_time = time.time()\n",
    "inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=100,\n",
    "        use_cache=True,\n",
    "        output_attentions=True,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "output_text = processor.decode(output_ids['sequences'][0], skip_special_tokens=False)\n",
    "print(output_text)\n",
    "inputs = processor(output_text, raw_image, return_tensors='pt').to(0, torch.float16)\n",
    "with torch.inference_mode():\n",
    "    output = model(**inputs, output_attentions=True, return_dict = True)\n",
    "end_time = time.time()\n",
    "vit_attention = model.vit_attentions\n",
    "vit_to_llm_mapping = output.vit_to_llm_mapping\n",
    "llm_attention = output.attentions\n",
    "\n",
    "# print the time\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_top_npercents_contribution(vit_attention, topk=topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_top_nprcnt_contribution_llm(llm_attention, vision_tokens_index=vit_to_llm_mapping[0], topk=topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

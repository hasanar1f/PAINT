{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as Colormap\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def cosine_sim(x, y):\n",
    "    # x: embedding torch.tensor [1024]\n",
    "    # y: embedding torch.tensor [1024]\n",
    "    return (x @ y) / (x.norm() * y.norm())\n",
    "\n",
    "def vision_token_redundency(image_features, num_window, start_i):\n",
    "    # image_features: torch.tensor [1, num_tokens, embedding_dim]\n",
    "    # num_window: int\n",
    "    # start_i: int\n",
    "    image_features = image_features[0, :, :]  # [num_tokens, embedding_dim]\n",
    "    similarity_matrix = np.zeros((num_window, num_window))\n",
    "\n",
    "    # Calculate cosine similarity for the specified window\n",
    "    for i in range(num_window):\n",
    "        for j in range(num_window):\n",
    "            similarity = cosine_sim(image_features[start_i+i],image_features[start_i+j])\n",
    "            similarity_matrix[i, j] = similarity.item()  # Convert tensor to a scalar\n",
    "\n",
    "    # Plot the similarity matrix using a heatmap\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(similarity_matrix, cmap='viridis')\n",
    "    plt.title('Cosine Similarity')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('token index')\n",
    "    plt.ylabel('token index')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_accumulated_attn_by_vision_token(attn, vision_tokens_index):\n",
    "    # attn: tuple of torch tensor [1, num_heads, n_all_tokens, n_all_tokens]\n",
    "    # vision_tokens_index: torch tensor [n_vision_tokens]\n",
    "    num_layers = len(attn)\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        layer_attn = attn[layer][0]  # [num_heads, n_all_tokens, n_all_tokens]\n",
    "        layer_attn = torch.sum(layer_attn, dim=0)  # [n_all_tokens, n_all_tokens]\n",
    "        vision_attn = layer_attn[: , vision_tokens_index]  # [n_vision_tokens, n_all_tokens]\n",
    "\n",
    "        sum_vision_attn = torch.sum(vision_attn, dim=0).sum()\n",
    "        sum_full_attn = torch.sum(layer_attn, dim=0).sum()\n",
    "\n",
    "        acc_attn_by_vision_token = sum_vision_attn / sum_full_attn\n",
    "        print(f\"Acc attn in Layer {layer}: {acc_attn_by_vision_token:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def visualize_vit_attention(vit_attention,layer):\n",
    "    attn = vit_attention[layer][0] # [num_heads, n_all_tokens, n_all_tokens]\n",
    "    attn = attn[:,0,1:] # [num_heads,n_all_tokens]\n",
    "    attn = attn.mean(dim=0) # [n_all_tokens]\n",
    "\n",
    "    num_vision_tokens = attn.shape[-1]\n",
    "    num_grid = int(np.sqrt(num_vision_tokens))\n",
    "\n",
    "    attn = attn.cpu().numpy().reshape((num_grid,num_grid))\n",
    "    # Plotting the attention heatmap\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(attn, annot=False, cmap='viridis', norm=LogNorm(vmin=attn.min()+1e-8, vmax=attn.max()),cbar=False)\n",
    "    plt.title(f'Layer {layer+1}\\'s Vision Tokens Attention Map')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_vision_attention_in_llm(llm_attention, layer=0, vit_to_llm_mapping=[], mark_topk=0):\n",
    "    num_vision_tokens = len(vit_to_llm_mapping)\n",
    "    text_token_start = vit_to_llm_mapping[-1] + 1\n",
    "    num_grid = int(np.ceil(np.sqrt(num_vision_tokens)))\n",
    "    attn = llm_attention[layer][0]  # [num_heads, n_all_tokens, n_all_tokens]\n",
    "    vision_attn = attn[:, text_token_start:, vit_to_llm_mapping]  # [num_heads, n_text_tokens, n_vision_tokens]\n",
    "    vision_attn = vision_attn.mean(dim=0)  # [n_text_tokens, n_vision_tokens]\n",
    "    vision_attn = vision_attn.mean(dim=0)  # [n_vision_tokens]\n",
    "    vision_attn = vision_attn.cpu().numpy().reshape((num_grid, num_grid))\n",
    "    \n",
    "    # Plotting the attention heatmap\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(vision_attn, annot=False, cmap='viridis', norm=LogNorm(vmin=0.00009, vmax=vision_attn.max()),cbar=False)\n",
    "    plt.title(f'Layer {layer+1}\\'s Vision Tokens Attention Map')\n",
    "\n",
    "    # if mark_topk > 0:\n",
    "    #     flat_indices = np.argpartition(vision_attn.flatten(), -mark_topk)[-mark_topk:]  # Get indices of top-k values\n",
    "    #     topk_indices = np.array(np.unravel_index(flat_indices, vision_attn.shape)).T  # Convert flat indices to 2D indices\n",
    "    #     # Plot a red box around top-k patches\n",
    "    #     for idx in topk_indices:\n",
    "    #         plt.gca().add_patch(plt.Rectangle((idx[1]-0.5, idx[0]-0.5), 1, 1, fill=False, edgecolor='red', lw=2))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_attention(multihead_attention, layer=31, stride=1, vision_tokens_index=[]):\n",
    "\n",
    "    multihead_attention = multihead_attention[layer].cpu()  # Shape: (1, num_heads, n_tokens, n_tokens)\n",
    "    averaged_attention = torch.mean(multihead_attention, dim=1)[0].float()  # Shape: (n_tokens, n_tokens)\n",
    "    averaged_attention = torch.nn.functional.avg_pool2d(averaged_attention.unsqueeze(0).unsqueeze(0), stride, stride).squeeze(0).squeeze(0)\n",
    "    cmap = plt.cm.get_cmap(\"viridis\")\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(5, 5), dpi=100)\n",
    "    log_norm = LogNorm(vmin=0.0007, vmax=averaged_attention.max())\n",
    "    ax = sns.heatmap(averaged_attention, cmap=cmap, norm=log_norm)\n",
    "\n",
    "    ax.set_xlabel('Token Index')\n",
    "    ax.set_ylabel('Token Index')\n",
    "\n",
    "    # do not show ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Title\n",
    "    plt.title(f'Attention Map Visualization for {layer+1}th layer')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llava-hf/llava-v1.6-vicuna-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava_next to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\n",
      "/home/hasan/kvto_vlm/transformers/src/transformers/models/llava/configuration_llava.py:101: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    attn_implementation=\"eager\",\n",
    ").to(0)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run down there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image successfully retrieved and saved.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://picsum.photos/400\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open('image.jpg', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Image successfully retrieved and saved.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve image. HTTP Status code: {response.status_code}\")\n",
    "raw_image = Image.open('../billboard.jpg')\n",
    "# raw_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"USER: <image>\\nTell me the story of two friends and the bear\\nASSISTANT:\"\n",
    "prompt = \"USER: <image>\\nWhat is the main text written on the billboard?\\nASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.fast_vlm_config = {\n",
    "    \"spatial_budget\": 0,\n",
    "    \"alpha_vision_token_budget\": 0.5,\n",
    "    \"beta_sub_images_budget\": 0.5,\n",
    "    \"clip_attn_layer\": 22,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['image_sizes'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(prompt, raw_image, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m, torch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 6\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_text)\n",
      "File \u001b[0;32m~/miniconda3/envs/kvto/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kvto_vlm/transformers/src/transformers/generation/utils.py:1633\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[1;32m   1632\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1633\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model)\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m~/kvto_vlm/transformers/src/transformers/generation/utils.py:1233\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1230\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1236\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['image_sizes'] (note: typos in the generate arguments will also show up in this list)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# start time\n",
    "start_time = time.time()\n",
    "inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=250,\n",
    "        use_cache=True,\n",
    "        output_attentions=True,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "output_text = processor.decode(output_ids['sequences'][0], skip_special_tokens=False)\n",
    "print(output_text)\n",
    "inputs = processor(output_text, raw_image, return_tensors='pt').to(0, torch.float16)\n",
    "with torch.inference_mode():\n",
    "    output = model(**inputs, output_attentions=True, return_dict = True)\n",
    "end_time = time.time()\n",
    "vit_attention = model.vit_attentions\n",
    "vit_to_llm_mapping = model.vit_to_llm_mapping\n",
    "llm_attention = output.attentions\n",
    "image_features = model.image_features\n",
    "# print the time\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention(multihead_attention=llm_attention,layer=9,stride=1,vision_tokens_index=vit_to_llm_mapping[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vision_token_redundency(image_features=image_features,num_window=200,start_i=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
